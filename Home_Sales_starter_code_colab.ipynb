{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KW73O2e3dw",
        "outputId": "5678f9ac-1a45-4c30-9ba0-cdaae73503de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [1 I\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [860 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,136 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [980 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,241 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 4,558 kB in 3s (1,528 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.4.0'\n",
        "spark_version = 'spark-3.4.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1. Read in the AWS S3 bucket into a DataFrame\n",
        "\n",
        "### Importing Necessary Packages\n",
        "First, we import the essential packages needed for the task.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "```\n",
        "\n",
        "### Creating a SparkSession\n",
        "SparkSession is the entry point to any Spark functionality. When you create a SparkSession, it initiates a Spark Application that all the code for that Session will run on.\n",
        "\n",
        "```python\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "spark\n",
        "```\n",
        "\n",
        "### Defining the S3 Bucket URL\n",
        "This URL points to the CSV file in the S3 bucket that you want to read into a DataFrame.\n",
        "\n",
        "```python\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "```\n",
        "\n",
        "### Adding the URL to Spark Context\n",
        "Spark context is the client-side object that represents the connection to a Spark cluster and is used to configure and connect to your Spark cluster.\n",
        "\n",
        "```python\n",
        "spark.sparkContext.addFile(url)\n",
        "```\n",
        "\n",
        "### Reading the CSV file into a DataFrame\n",
        "Spark DataFrame is a distributed collection of data organized into named columns. Here, we're reading the CSV file from the Spark context, specifying the separator as \",\" and treating the first row as headers.\n",
        "\n",
        "```python\n",
        "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep = \",\" , header = True)\n",
        "```\n",
        "\n",
        "### Showing the DataFrame\n",
        "This will print the first 20 rows of the DataFrame in the console.\n",
        "\n",
        "```python\n",
        "df.show()\n",
        "```\n"
      ],
      "metadata": {
        "id": "bFOwZR1JjS7J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2XbWNf1Te5fM",
        "outputId": "36027a3d-73df-4b50-a44b-41893b97ab72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7940382cc760>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d137bbf74298:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SparkSQL</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import packages\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wOJqxG_RPSwp",
        "outputId": "d571ac68-4fde-40c5-9981-b6d066a3e1cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
            "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
            "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
            "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
            "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
            "|5aa00529-0533-46b...|2019-01-30|      2017|218712|       2|        3|       1965|   14375|     2|         0|   7|\n",
            "|131492a1-72e2-4a8...|2020-02-08|      2017|419199|       2|        3|       2062|    8876|     2|         0|   6|\n",
            "|8d54a71b-c520-44e...|2019-07-21|      2010|323956|       2|        3|       1506|   11816|     1|         0|  25|\n",
            "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
            "|2ed8d509-7372-46d...|2021-08-06|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
            "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
            "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527|       2|        3|       1926|   12556|     1|         0|  23|\n",
            "|941bad30-eb49-4a7...|2020-05-09|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
            "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247|       3|        2|       1672|   11986|     2|         0|  28|\n",
            "|f1e4cef7-d151-439...|2019-02-01|      2011|398667|       2|        3|       2331|   11356|     1|         0|   7|\n",
            "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
            "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
            "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650|       2|        3|       2100|   10419|     2|         0|   7|\n",
            "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
            "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
        "\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep = \",\" , header = True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Create a temporary view of the DataFrame.\n",
        "df.createOrReplaceTempView(\"sales\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark is an open-source distributed computing system, and `spark.sql` is a part of Spark's SQL module. It allows you to run SQL queries programmatically, returning the result as a DataFrame. This can be quite powerful for data manipulation and querying in a big data context.\n",
        "\n",
        "Here's a brief breakdown:\n",
        "\n",
        "- `spark`: When you're working with Spark, you typically start by creating a SparkSession. This is usually stored in a variable named `spark`, and it's the entry point to any functionality in Spark.\n",
        "\n",
        "- `sql`: This method on the SparkSession allows you to run SQL queries just as you would against a traditional database. You pass in your SQL query as a string, and it returns a DataFrame with the results.\n",
        "\n",
        "So when you see `spark.sql('SELECT * FROM table')`, this is running a SQL query to select everything from the specified table and returning it as a DataFrame, allowing you to manipulate and analyze the data using Spark's distributed computing capabilities.\n",
        "\n",
        "It's a handy tool to have, especially if you're already comfortable with SQL, as it allows you to leverage those skills in a distributed computing environment like Spark. It can be particularly useful in data analysis tasks like the ones you're learning in your bootcamp."
      ],
      "metadata": {
        "id": "hK_aQ-8ELAhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is the average price for a four-bedroom house sold in each year rounded to two decimal places?**"
      ],
      "metadata": {
        "id": "tzRXoBagLTUd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L6fkwOeOmqvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abce668e-9369-421f-df4c-bfb40e3bc1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|year|average_price|\n",
            "+----+-------------+\n",
            "|2022|    296363.88|\n",
            "|2019|     300263.7|\n",
            "|2020|    298353.78|\n",
            "|2021|    301819.44|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  YEAR(date) AS year,\n",
        "  ROUND(AVG(price),2 ) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "WHERE\n",
        "  bedrooms = 4\n",
        "GROUP BY\n",
        "  YEAR(date)\n",
        "\n",
        "''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?**"
      ],
      "metadata": {
        "id": "gj-QPRk-La7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l8p_tUS8h8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9bd74c-b693-408e-b1f6-773a0064f193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|year|average_price|\n",
            "+----+-------------+\n",
            "|2022|    290780.14|\n",
            "|2019|    292438.24|\n",
            "|2020|    289659.97|\n",
            "|2021|    292627.68|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  YEAR(date) AS year,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "WHERE\n",
        "  bedrooms = 4 AND bathrooms = 3\n",
        "GROUP BY\n",
        "  YEAR(date)\n",
        "''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query.**"
      ],
      "metadata": {
        "id": "aAHc3YzvLboW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y-Eytz64liDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1e6b17-91d1-4849-a28b-13fcca8769bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|year|average_price|\n",
            "+----+-------------+\n",
            "|2022|    295505.36|\n",
            "|2019|    301672.15|\n",
            "|2020|    289891.27|\n",
            "|2021|    296672.69|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
        "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
        "\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  YEAR(date) AS year,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "WHERE\n",
        "  bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
        "GROUP BY\n",
        "  YEAR(date)\n",
        "''').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "0d220af0-1619-4772-aa95-e8c60832a1d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|rating|average_price|\n",
            "+------+-------------+\n",
            "|    99|   1061201.42|\n",
            "|    98|   1053739.33|\n",
            "|    97|   1129040.15|\n",
            "|    96|   1017815.92|\n",
            "|    95|    1054325.6|\n",
            "|    94|    1033536.2|\n",
            "|    93|   1026006.06|\n",
            "|    92|    970402.55|\n",
            "|    91|   1137372.73|\n",
            "|    90|   1062654.16|\n",
            "|    89|   1107839.15|\n",
            "|    88|   1031719.35|\n",
            "|    87|    1072285.2|\n",
            "|    86|   1070444.25|\n",
            "|    85|   1056336.74|\n",
            "|    84|   1117233.13|\n",
            "|    83|   1033965.93|\n",
            "|    82|    1063498.0|\n",
            "|    81|   1053472.79|\n",
            "|    80|    991767.38|\n",
            "+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.9454669952392578 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
        "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
        "\n",
        "start_time = time.time()\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  view as rating,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "GROUP BY\n",
        "  view\n",
        "HAVING\n",
        "  AVG(price) >= 350000\n",
        "ORDER BY\n",
        "  view desc\n",
        "\n",
        "''').show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KAhk3ZD2tFy8"
      },
      "outputs": [],
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.catalog.cacheTable(\"sales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caching** is a technique used in computing to store data in a place where it can be accessed more quickly. It's like keeping your favorite snacks in a nearby drawer instead of in the kitchen – they're easier to grab when you want them.\n",
        "\n",
        "\n",
        "**What is Cached in spark:** In the context of Spark, caching means storing the result of a DataFrame or temporary view in memory. This allows Spark to access the data more quickly than if it had to read it from disk every time."
      ],
      "metadata": {
        "id": "knvbtVzdmYw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4opVhbvxtL-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea83bb6-f79d-4fab-f422-6b268cfb16a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# 8. Check if the table is cached.\n",
        "spark.catalog.isCached('sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnL46lwTSEk",
        "outputId": "048d4337-3f92-49d1-f85a-e1c5633cd146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|rating|average_price|\n",
            "+------+-------------+\n",
            "|    99|   1061201.42|\n",
            "|    98|   1053739.33|\n",
            "|    97|   1129040.15|\n",
            "|    96|   1017815.92|\n",
            "|    95|    1054325.6|\n",
            "|    94|    1033536.2|\n",
            "|    93|   1026006.06|\n",
            "|    92|    970402.55|\n",
            "|    91|   1137372.73|\n",
            "|    90|   1062654.16|\n",
            "|    89|   1107839.15|\n",
            "|    88|   1031719.35|\n",
            "|    87|    1072285.2|\n",
            "|    86|   1070444.25|\n",
            "|    85|   1056336.74|\n",
            "|    84|   1117233.13|\n",
            "|    83|   1033965.93|\n",
            "|    82|    1063498.0|\n",
            "|    81|   1053472.79|\n",
            "|    80|    991767.38|\n",
            "+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.45435190200805664 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 9. Using the cached data, run the query that filters out the view ratings with average price\n",
        "#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
        "\n",
        "start_time = time.time()\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  view as rating,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "GROUP BY\n",
        "  view\n",
        "HAVING\n",
        "  AVG(price) >= 350000\n",
        "ORDER BY\n",
        "  view desc\n",
        "\n",
        "\n",
        "''').show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "df.write.partitionBy(\"date_built\").mode(\"overwrite\").parquet(\"parqet_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AZ7BgY61sRqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1933b61-1a9e-4ed6-ee8f-2b6bfafdf8cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(id='2ed8d509-7372-46d5-a9dd-9281a95467d4', date='2021-08-06', price='258710', bedrooms='3', bathrooms='3', sqft_living='1918', sqft_lot='9666', floors='1', waterfront='0', view='25', date_built=2015)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# 11. Read the parquet formatted data.\n",
        "parqet_sales_df = spark.read.parquet(\"parqet_sales\")\n",
        "parqet_sales_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# 12. Create a temporary table for the parquet data.\n",
        "parqet_sales_df.createOrReplaceTempView('parqet_sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "12a0f672-8cf6-4377-ab2e-afa446b85778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|rating|average_price|\n",
            "+------+-------------+\n",
            "|    99|   1061201.42|\n",
            "|    98|   1053739.33|\n",
            "|    97|   1129040.15|\n",
            "|    96|   1017815.92|\n",
            "|    95|    1054325.6|\n",
            "|    94|    1033536.2|\n",
            "|    93|   1026006.06|\n",
            "|    92|    970402.55|\n",
            "|    91|   1137372.73|\n",
            "|    90|   1062654.16|\n",
            "|    89|   1107839.15|\n",
            "|    88|   1031719.35|\n",
            "|    87|    1072285.2|\n",
            "|    86|   1070444.25|\n",
            "|    85|   1056336.74|\n",
            "|    84|   1117233.13|\n",
            "|    83|   1033965.93|\n",
            "|    82|    1063498.0|\n",
            "|    81|   1053472.79|\n",
            "|    80|    991767.38|\n",
            "+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.3203420639038086 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000\n",
        "# with the parquet DataFrame. Round your average to two decimal places.\n",
        "# Determine the runtime and compare it to the cached version.\n",
        "start_time = time.time()\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  view as rating,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "GROUP BY\n",
        "  view\n",
        "HAVING\n",
        "  AVG(price) >= 350000\n",
        "ORDER BY\n",
        "  view desc\n",
        "\n",
        "''').show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjjYzQGjtbq8"
      },
      "outputs": [],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy9NBvO7tlmm"
      },
      "outputs": [],
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si-BNruRUGK3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}