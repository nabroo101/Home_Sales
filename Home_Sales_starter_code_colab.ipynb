{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KW73O2e3dw",
        "outputId": "08ad6b78-8da8-4f01-f904-5653de03248f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 12.7 kB/110 k\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 17.1 kB/110 kB 15%] [2 InRelease 0 B/3,62\r0% [Waiting for headers] [1 InRelease 34.4 kB/110 kB 31%] [Connecting to ppa.la\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 43.1 kB/110 kB 39%] [Connecting to ppa.la\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [983 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,136 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [860 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,247 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [37.1 kB]\n",
            "Fetched 4,629 kB in 3s (1,743 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.4.0'\n",
        "spark_version = 'spark-3.4.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1. Read in the AWS S3 bucket into a DataFrame\n",
        "\n",
        "### Importing Necessary Packages\n",
        "First, we import the essential packages needed for the task.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "```\n",
        "\n",
        "### Creating a SparkSession\n",
        "SparkSession is the entry point to any Spark functionality. When you create a SparkSession, it initiates a Spark Application that all the code for that Session will run on.\n",
        "\n",
        "```python\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "spark\n",
        "```\n",
        "\n",
        "### Defining the S3 Bucket URL\n",
        "This URL points to the CSV file in the S3 bucket that you want to read into a DataFrame.\n",
        "\n",
        "```python\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "```\n",
        "\n",
        "### Adding the URL to Spark Context\n",
        "Spark context is the client-side object that represents the connection to a Spark cluster and is used to configure and connect to your Spark cluster.\n",
        "\n",
        "```python\n",
        "spark.sparkContext.addFile(url)\n",
        "```\n",
        "\n",
        "### Reading the CSV file into a DataFrame\n",
        "Spark DataFrame is a distributed collection of data organized into named columns. Here, we're reading the CSV file from the Spark context, specifying the separator as \",\" and treating the first row as headers.\n",
        "\n",
        "```python\n",
        "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep = \",\" , header = True)\n",
        "```\n",
        "\n",
        "### Showing the DataFrame\n",
        "This will print the first 20 rows of the DataFrame in the console.\n",
        "\n",
        "```python\n",
        "df.show()\n",
        "```\n"
      ],
      "metadata": {
        "id": "bFOwZR1JjS7J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2XbWNf1Te5fM",
        "outputId": "14ad3898-9f3f-412c-a00c-347475f17b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b47e3ffb460>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://183fb90952ee:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SparkSQL</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import packages\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wOJqxG_RPSwp",
        "outputId": "1e726cdc-0a14-470e-fd83-7e004e2d8bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
            "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
            "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
            "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
            "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
            "|5aa00529-0533-46b...|2019-01-30|      2017|218712|       2|        3|       1965|   14375|     2|         0|   7|\n",
            "|131492a1-72e2-4a8...|2020-02-08|      2017|419199|       2|        3|       2062|    8876|     2|         0|   6|\n",
            "|8d54a71b-c520-44e...|2019-07-21|      2010|323956|       2|        3|       1506|   11816|     1|         0|  25|\n",
            "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
            "|2ed8d509-7372-46d...|2021-08-06|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
            "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
            "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527|       2|        3|       1926|   12556|     1|         0|  23|\n",
            "|941bad30-eb49-4a7...|2020-05-09|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
            "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247|       3|        2|       1672|   11986|     2|         0|  28|\n",
            "|f1e4cef7-d151-439...|2019-02-01|      2011|398667|       2|        3|       2331|   11356|     1|         0|   7|\n",
            "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
            "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
            "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650|       2|        3|       2100|   10419|     2|         0|   7|\n",
            "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
            "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
        "\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep = \",\" , header = True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Create a temporary view of the DataFrame.\n",
        "df.createOrReplaceTempView(\"sales\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark is an open-source distributed computing system, and `spark.sql` is a part of Spark's SQL module. It allows you to run SQL queries programmatically, returning the result as a DataFrame. This can be quite powerful for data manipulation and querying in a big data context.\n",
        "\n",
        "Here's a brief breakdown:\n",
        "\n",
        "- `spark`: When you're working with Spark, you typically start by creating a SparkSession. This is usually stored in a variable named `spark`, and it's the entry point to any functionality in Spark.\n",
        "\n",
        "- `sql`: This method on the SparkSession allows you to run SQL queries just as you would against a traditional database. You pass in your SQL query as a string, and it returns a DataFrame with the results.\n",
        "\n",
        "So when you see `spark.sql('SELECT * FROM table')`, this is running a SQL query to select everything from the specified table and returning it as a DataFrame, allowing you to manipulate and analyze the data using Spark's distributed computing capabilities.\n",
        "\n",
        "It's a handy tool to have, especially if you're already comfortable with SQL, as it allows you to leverage those skills in a distributed computing environment like Spark. It can be particularly useful in data analysis tasks like the ones you're learning in your bootcamp."
      ],
      "metadata": {
        "id": "hK_aQ-8ELAhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is the average price for a four-bedroom house sold in each year rounded to two decimal places?**"
      ],
      "metadata": {
        "id": "tzRXoBagLTUd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L6fkwOeOmqvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8e20f9-5161-4051-fe7e-fe38a6e4a3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|year|average_price|\n",
            "+----+-------------+\n",
            "|2022|    296363.88|\n",
            "|2019|     300263.7|\n",
            "|2020|    298353.78|\n",
            "|2021|    301819.44|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  YEAR(date) AS year,\n",
        "  ROUND(AVG(price),2 ) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "WHERE\n",
        "  bedrooms = 4\n",
        "GROUP BY\n",
        "  YEAR(date)\n",
        "\n",
        "''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?**"
      ],
      "metadata": {
        "id": "gj-QPRk-La7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l8p_tUS8h8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318cd514-ecb3-4af3-9488-3b1ac687103a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|year|average_price|\n",
            "+----+-------------+\n",
            "|2022|    290780.14|\n",
            "|2019|    292438.24|\n",
            "|2020|    289659.97|\n",
            "|2021|    292627.68|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  YEAR(date) AS year,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "WHERE\n",
        "  bedrooms = 4 AND bathrooms = 3\n",
        "GROUP BY\n",
        "  YEAR(date)\n",
        "''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query.**"
      ],
      "metadata": {
        "id": "aAHc3YzvLboW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y-Eytz64liDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2316e5f-b909-491c-f08f-265e7ae29d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|year|average_price|\n",
            "+----+-------------+\n",
            "|2022|    290242.99|\n",
            "|2019|    289859.14|\n",
            "|2020|    292289.09|\n",
            "|2021|    296330.96|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
        "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
        "\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  YEAR(date) AS year,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "WHERE\n",
        "  bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
        "GROUP BY\n",
        "  YEAR(date)\n",
        "''').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "1b53f6b9-1a28-4c07-d035-d26926bf1e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|rating|average_price|\n",
            "+------+-------------+\n",
            "|    99|   1061201.42|\n",
            "|    98|   1053739.33|\n",
            "|    97|   1129040.15|\n",
            "|    96|   1017815.92|\n",
            "|    95|    1054325.6|\n",
            "|    94|    1033536.2|\n",
            "|    93|   1026006.06|\n",
            "|    92|    970402.55|\n",
            "|    91|   1137372.73|\n",
            "|    90|   1062654.16|\n",
            "|    89|   1107839.15|\n",
            "|    88|   1031719.35|\n",
            "|    87|    1072285.2|\n",
            "|    86|   1070444.25|\n",
            "|    85|   1056336.74|\n",
            "|    84|   1117233.13|\n",
            "|    83|   1033965.93|\n",
            "|    82|    1063498.0|\n",
            "|    81|   1053472.79|\n",
            "|    80|    991767.38|\n",
            "+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.9553377628326416 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
        "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
        "\n",
        "start_time = time.time()\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  view as rating,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "GROUP BY\n",
        "  view\n",
        "HAVING\n",
        "  AVG(price) >= 350000\n",
        "ORDER BY\n",
        "  view desc\n",
        "\n",
        "''').show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KAhk3ZD2tFy8"
      },
      "outputs": [],
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.catalog.cacheTable(\"sales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caching** is a technique used in computing to store data in a place where it can be accessed more quickly. It's like keeping your favorite snacks in a nearby drawer instead of in the kitchen – they're easier to grab when you want them.\n",
        "\n",
        "\n",
        "**What is Cached in spark:** In the context of Spark, caching means storing the result of a DataFrame or temporary view in memory. This allows Spark to access the data more quickly than if it had to read it from disk every time."
      ],
      "metadata": {
        "id": "knvbtVzdmYw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4opVhbvxtL-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fffb54-7642-4da4-ae83-ab63be347c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 8. Check if the table is cached.\n",
        "spark.catalog.isCached('sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnL46lwTSEk",
        "outputId": "639e9a5b-9692-4589-f748-aea24693fefc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|rating|average_price|\n",
            "+------+-------------+\n",
            "|    99|   1061201.42|\n",
            "|    98|   1053739.33|\n",
            "|    97|   1129040.15|\n",
            "|    96|   1017815.92|\n",
            "|    95|    1054325.6|\n",
            "|    94|    1033536.2|\n",
            "|    93|   1026006.06|\n",
            "|    92|    970402.55|\n",
            "|    91|   1137372.73|\n",
            "|    90|   1062654.16|\n",
            "|    89|   1107839.15|\n",
            "|    88|   1031719.35|\n",
            "|    87|    1072285.2|\n",
            "|    86|   1070444.25|\n",
            "|    85|   1056336.74|\n",
            "|    84|   1117233.13|\n",
            "|    83|   1033965.93|\n",
            "|    82|    1063498.0|\n",
            "|    81|   1053472.79|\n",
            "|    80|    991767.38|\n",
            "+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 1.73494553565979 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 9. Using the cached data, run the query that filters out the view ratings with average price\n",
        "#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
        "\n",
        "start_time = time.time()\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  view as rating,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "GROUP BY\n",
        "  view\n",
        "HAVING\n",
        "  AVG(price) >= 350000\n",
        "ORDER BY\n",
        "  view desc\n",
        "\n",
        "\n",
        "''').show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 10. Partition the Data by \"date_built\"\n",
        "Here, we are writing the DataFrame `df` to a parquet file and partitioning it by the \"date_built\" field. Partitioning the data this way can optimize read performance for queries involving this field.\n",
        "```python\n",
        "df.write.partitionBy(\"date_built\").mode(\"overwrite\").parquet(\"parqet_sales\")\n",
        "```\n",
        "\n",
        "### 11. Read the Parquet Formatted Data\n",
        "Next, we read the parquet file that was just written and assign it to a new DataFrame. We can then display the first few rows with the `head()` method to quickly check the data.\n",
        "```python\n",
        "parqet_sales_df = spark.read.parquet(\"parqet_sales\")\n",
        "parqet_sales_df.head()\n",
        "```\n",
        "\n",
        "### 12. Create a Temporary Table for the Parquet Data\n",
        "Finally, we create a temporary table view of the parquet data. This allows us to query the DataFrame using SQL syntax, offering more flexibility and convenience for analysis.\n",
        "```python\n",
        "parqet_sales_df.createOrReplaceTempView('parqet_sales')\n"
      ],
      "metadata": {
        "id": "lo62dcz5YLuq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "df.write.partitionBy(\"date_built\").mode(\"overwrite\").parquet(\"parqet_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AZ7BgY61sRqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fa63ff-358b-487a-c38c-d6ff15b408ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(id='2ed8d509-7372-46d5-a9dd-9281a95467d4', date='2021-08-06', price='258710', bedrooms='3', bathrooms='3', sqft_living='1918', sqft_lot='9666', floors='1', waterfront='0', view='25', date_built=2015)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# 11. Read the parquet formatted data.\n",
        "parqet_sales_df = spark.read.parquet(\"parqet_sales\")\n",
        "parqet_sales_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# 12. Create a temporary table for the parquet data.\n",
        "parqet_sales_df.createOrReplaceTempView('parqet_sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "4574f197-3102-47cc-edc0-dfdb5ec3f1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|rating|average_price|\n",
            "+------+-------------+\n",
            "|    99|   1061201.42|\n",
            "|    98|   1053739.33|\n",
            "|    97|   1129040.15|\n",
            "|    96|   1017815.92|\n",
            "|    95|    1054325.6|\n",
            "|    94|    1033536.2|\n",
            "|    93|   1026006.06|\n",
            "|    92|    970402.55|\n",
            "|    91|   1137372.73|\n",
            "|    90|   1062654.16|\n",
            "|    89|   1107839.15|\n",
            "|    88|   1031719.35|\n",
            "|    87|    1072285.2|\n",
            "|    86|   1070444.25|\n",
            "|    85|   1056336.74|\n",
            "|    84|   1117233.13|\n",
            "|    83|   1033965.93|\n",
            "|    82|    1063498.0|\n",
            "|    81|   1053472.79|\n",
            "|    80|    991767.38|\n",
            "+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.3086867332458496 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000\n",
        "# with the parquet DataFrame. Round your average to two decimal places.\n",
        "# Determine the runtime and compare it to the cached version.\n",
        "start_time = time.time()\n",
        "spark.sql('''\n",
        "SELECT\n",
        "  view as rating,\n",
        "  ROUND(AVG(price), 2) AS average_price\n",
        "FROM\n",
        "  sales\n",
        "GROUP BY\n",
        "  view\n",
        "HAVING\n",
        "  AVG(price) >= 350000\n",
        "ORDER BY\n",
        "  view desc\n",
        "\n",
        "''').show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 14. Uncache the parqet_sales Temporary Table\n",
        "Here, we use a SQL command to uncache the temporary table `parqet_sales`. Uncaching a table removes it from memory, which can free up resources.\n",
        "```python\n",
        "spark.sql(\"uncache table parqet_sales\")\n",
        "```\n",
        "\n",
        "### 15. Check if the parqet_sales Table is No Longer Cached\n",
        "We then check whether the `parqet_sales` table is still cached or not. If the table is still cached, it will print a warning message. If not, it will print \"all clear,\" confirming that the resources have been freed.\n",
        "```python\n",
        "if spark.catalog.isCached(\"parqet_sales\"):\n",
        "  print(\"a table is still cached\")\n",
        "else:\n",
        "  print(\"all clear\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "0dvTO8ZrZJEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hjjYzQGjtbq8",
        "outputId": "f0149ab2-8f71-43bf-91a6-ced24b35474f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.sql(\"uncache table parqet_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Sy9NBvO7tlmm",
        "outputId": "30188d1b-4693-4635-c4fa-4ab4f0574464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all clear\n"
          ]
        }
      ],
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "if spark.catalog.isCached(\"parqet_sales\"):\n",
        "  print(\"a table is till cached\")\n",
        "else:\n",
        "  print(\"all clear\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si-BNruRUGK3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}